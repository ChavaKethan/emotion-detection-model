# -*- coding: utf-8 -*-
"""Copy of Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DdPQPAu73_X7_pc_cZt3WHJQvmTGvGNs
"""

import pickle
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.utils import resample
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint
import shap
import joblib
import tensorflow as tf
import gc
from tensorflow.keras.backend import clear_session

# Step 1: Load WESAD Dataset
file_path = '/content/drive/MyDrive/WESAD/WESAD/S2/S2.pkl'
with open(file_path, 'rb') as f:
    data = pickle.load(f, encoding='latin1')

signals = data['signal']
labels = data['label']
eda = signals['chest']['EDA']
ecq = signals['chest']['ECG']
temp = signals['chest'].get('TEMP', signals['chest'].get('Temp'))

if temp is None:
    raise ValueError("Temperature data not found.")

# Step 2: Create DataFrame
df = pd.DataFrame({
    'EDA': eda.flatten(),
    'HRV': ecq.flatten(),
    'TEMP': temp.flatten(),
    'Label': labels
})

# Step 3: Map Labels to Emotions
emotion_mapping = {
    1: 'Focus',
    2: 'Anxiety',
    3: 'Happy'
}

df['Emotion'] = df['Label'].map(emotion_mapping)

# Step 4: Remove Undefined Data
df = df.dropna(subset=['Emotion'])
print(df['Emotion'].value_counts())

# Step 5: Balance Dataset Across Three Emotions
emotion_dataframes = {emotion: df[df['Emotion'] == emotion] for emotion in ['Happy', 'Focus', 'Anxiety']}
min_samples = min(len(emotion_dataframes[emotion]) for emotion in emotion_dataframes)

balanced_df = pd.concat([
    resample(emotion_dataframes[emotion], replace=True, n_samples=min_samples, random_state=42)
    for emotion in emotion_dataframes
]).sample(frac=1, random_state=42).reset_index(drop=True)

print(balanced_df['Emotion'].value_counts())

# Step 6: Feature Extraction
def extract_features(signal, window_size=60, overlap=30):
    features = []
    for start in range(0, len(signal) - window_size, overlap):
        window = signal[start:start + window_size]
        features.append({
            'mean': np.mean(window),
            'std': np.std(window),
            'max': np.max(window),
            'min': np.min(window),
        })
    return features

eda_features = extract_features(balanced_df['EDA'])
hrv_features = extract_features(balanced_df['HRV'])
temp_features = extract_features(balanced_df['TEMP'])

features = pd.DataFrame(eda_features).add_prefix('EDA_')
features = pd.concat([features, pd.DataFrame(hrv_features).add_prefix('HRV_')], axis=1)
features = pd.concat([features, pd.DataFrame(temp_features).add_prefix('TEMP_')], axis=1)
features['Emotion'] = balanced_df['Emotion'].iloc[:len(features)].values

# Step 7: Encode Emotions and Split Dataset
le = LabelEncoder()
features['Emotion_Encoded'] = le.fit_transform(features['Emotion'])
X = features.drop(['Emotion', 'Emotion_Encoded'], axis=1)
y = features['Emotion_Encoded']

# Reduce dataset size for LSTM training
subset_size = 5000
X_subset = X.sample(n=subset_size, random_state=42)
y_subset = y.loc[X_subset.index]

# Prepare sequential data for LSTM
timesteps = 10
X_lstm, y_lstm = [], []

for i in range(len(X_subset) - timesteps):
    X_lstm.append(X_subset.values[i:i+timesteps])
    y_lstm.append(y_subset.iloc[i+timesteps])

X_lstm = np.array(X_lstm)
y_lstm = np.array(y_lstm)

# Train-test split
X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(
    X_lstm, y_lstm, test_size=0.2, random_state=42
)

# Step 8: LSTM Model
clear_session()
gc.collect()
physical_devices = tf.config.list_physical_devices('GPU')
for device in physical_devices:
    tf.config.experimental.set_memory_growth(device, True)

lstm_model = Sequential([
    LSTM(32, input_shape=(timesteps, X_train_lstm.shape[2]), return_sequences=False),
    Dropout(0.2),
    Dense(16, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')
])

lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the LSTM model
history = lstm_model.fit(
    X_train_lstm, y_train_lstm,
    epochs=10,
    batch_size=16,
    validation_split=0.2
)

# Evaluate LSTM model
lstm_eval = lstm_model.evaluate(X_test_lstm, y_test_lstm)
print("LSTM Test Loss:", lstm_eval[0])
print("LSTM Test Accuracy:", lstm_eval[1])

# Plot training and validation accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title("LSTM Training and Validation Accuracy")
plt.legend()
plt.show()

# Plot training and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title("LSTM Training and Validation Loss")
plt.legend()
plt.show()

# Confusion Matrix for LSTM
y_pred_lstm = np.argmax(lstm_model.predict(X_test_lstm), axis=1)
cm_lstm = confusion_matrix(y_test_lstm, y_pred_lstm)

disp_lstm = ConfusionMatrixDisplay(confusion_matrix=cm_lstm, display_labels=['Focus', 'Anxiety', 'Happy'])
disp_lstm.plot(cmap='Blues')
plt.title("LSTM Confusion Matrix")
plt.show()

import pickle
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.backend import clear_session
import tensorflow as tf
import gc
from sklearn.utils import resample

# Step 1: Dataset Preparation
subset_size = 5000  # Limit dataset size for LSTM
timesteps = 10

# Sequential Data for LSTM
X_subset = X.sample(n=subset_size, random_state=42)
y_subset = y.loc[X_subset.index]

X_lstm, y_lstm = [], []
for i in range(len(X_subset) - timesteps):
    X_lstm.append(X_subset.values[i:i+timesteps])
    y_lstm.append(y_subset.iloc[i+timesteps])

X_lstm = np.array(X_lstm)
y_lstm = np.array(y_lstm)

X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(
    X_lstm, y_lstm, test_size=0.2, random_state=42
)

# Step 2: Train LSTM Model
clear_session()
gc.collect()
physical_devices = tf.config.list_physical_devices('GPU')
for device in physical_devices:
    tf.config.experimental.set_memory_growth(device, True)

lstm_model = Sequential([
    LSTM(32, input_shape=(timesteps, X_train_lstm.shape[2]), return_sequences=False),
    Dropout(0.2),
    Dense(16, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')
])

lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history = lstm_model.fit(
    X_train_lstm, y_train_lstm,
    epochs=10,
    batch_size=16,
    validation_split=0.2
)

feature_extractor = Model(inputs=lstm_model.inputs, outputs=lstm_model.layers[-2].output) # Changed lstm_model.input to lstm_model.inputs
X_train_lstm_features = feature_extractor.predict(X_train_lstm)
X_test_lstm_features = feature_extractor.predict(X_test_lstm)


# Step 3: Train Random Forest on LSTM Features
rf_combined_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_combined_model.fit(X_train_lstm_features, y_train_lstm)

# Evaluate the Combined Model
y_pred_combined = rf_combined_model.predict(X_test_lstm_features)
combined_accuracy = accuracy_score(y_test_lstm, y_pred_combined)
print("Combined LSTM + RF Accuracy:", combined_accuracy)

# Step 4: Confusion Matrix for Combined Model
cm_combined = confusion_matrix(y_test_lstm, y_pred_combined)
disp_combined = ConfusionMatrixDisplay(confusion_matrix=cm_combined, display_labels=['Focus', 'Anxiety', 'Happy'])
disp_combined.plot(cmap='Blues')
plt.title("Combined LSTM + RF Confusion Matrix")
plt.show()

# Step 5: Compare LSTM and Combined Model Accuracy
print("LSTM Test Accuracy:", lstm_model.evaluate(X_test_lstm, y_test_lstm)[1])
print("Combined Model Accuracy:", combined_accuracy)

import pickle
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.utils import resample
import joblib

# Step 1: Load WESAD Dataset
file_path = '/content/drive/MyDrive/WESAD/WESAD/S2/S2.pkl'
with open(file_path, 'rb') as f:
    data = pickle.load(f, encoding='latin1')

signals = data['signal']
labels = data['label']
eda = signals['chest']['EDA']
ecq = signals['chest']['ECG']
temp = signals['chest'].get('TEMP', signals['chest'].get('Temp'))

if temp is None:
    raise ValueError("Temperature data not found.")

# Step 2: Create DataFrame
df = pd.DataFrame({
    'EDA': eda.flatten(),
    'HRV': ecq.flatten(),
    'TEMP': temp.flatten(),
    'Label': labels
})

# Step 3: Map Labels to Emotions
emotion_mapping = {
    1: 'Focus',
    2: 'Anxiety',
    3: 'Happy'
}

df['Emotion'] = df['Label'].map(emotion_mapping)

# Step 4: Remove Undefined Data
df = df.dropna(subset=['Emotion'])
print(df['Emotion'].value_counts())

# Step 5: Balance Dataset Across Three Emotions
emotion_dataframes = {emotion: df[df['Emotion'] == emotion] for emotion in ['Happy', 'Focus', 'Anxiety']}
min_samples = min(len(emotion_dataframes[emotion]) for emotion in emotion_dataframes)

balanced_df = pd.concat([
    resample(emotion_dataframes[emotion], replace=True, n_samples=min_samples, random_state=42)
    for emotion in emotion_dataframes
]).sample(frac=1, random_state=42).reset_index(drop=True)

print(balanced_df['Emotion'].value_counts())

# Step 6: Feature Extraction
def extract_features(signal, window_size=60, overlap=30):
    features = []
    for start in range(0, len(signal) - window_size, overlap):
        window = signal[start:start + window_size]
        features.append({
            'mean': np.mean(window),
            'std': np.std(window),
            'max': np.max(window),
            'min': np.min(window),
        })
    return features

eda_features = extract_features(balanced_df['EDA'])
hrv_features = extract_features(balanced_df['HRV'])
temp_features = extract_features(balanced_df['TEMP'])

features = pd.DataFrame(eda_features).add_prefix('EDA_')
features = pd.concat([features, pd.DataFrame(hrv_features).add_prefix('HRV_')], axis=1)
features = pd.concat([features, pd.DataFrame(temp_features).add_prefix('TEMP_')], axis=1)
features['Emotion'] = balanced_df['Emotion'].iloc[:len(features)].values

# Step 7: Encode Emotions and Split Dataset
le = LabelEncoder()
features['Emotion_Encoded'] = le.fit_transform(features['Emotion'])
X = features.drop(['Emotion', 'Emotion_Encoded'], axis=1)
y = features['Emotion_Encoded']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 8: Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)
print(classification_report(y_test, y_pred_rf, target_names=le.classes_))
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))

# Step 9: Confusion Matrix for Random Forest
cm_rf = confusion_matrix(y_test, y_pred_rf)
disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=['Focus', 'Anxiety', 'Happy'])
disp_rf.plot(cmap='Blues')
plt.title("Random Forest Confusion Matrix")
plt.show()

# Step 10: Save Random Forest Model
joblib.dump(rf_model, '/content/drive/MyDrive/WESAD/random_forest_model.pkl')

# Collect Evaluation Metrics
results = {
    "Model": ["Random Forest"],
    "Accuracy": [accuracy_score(y_test, y_pred_rf)],
    "Precision": [classification_report(y_test, y_pred_rf, output_dict=True)['weighted avg']['precision']],
    "Recall": [classification_report(y_test, y_pred_rf, output_dict=True)['weighted avg']['recall']],
    "F1-Score": [classification_report(y_test, y_pred_rf, output_dict=True)['weighted avg']['f1-score']]
}

# Add LSTM and Combined Model Results Manually (If you have them saved earlier)
results["Model"].extend(["LSTM", "Combined LSTM + RF"])
results["Accuracy"].extend([0.87, 0.89])  # Replace with your LSTM and Combined model accuracy
results["Precision"].extend([0.86, 0.88])
results["Recall"].extend([0.87, 0.89])
results["F1-Score"].extend([0.86, 0.88])

# Convert to DataFrame
import pandas as pd
results_df = pd.DataFrame(results)
print(results_df)

import matplotlib.pyplot as plt

# Plot Accuracy, Precision, Recall, F1-Score
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
x = range(len(results["Model"]))

plt.figure(figsize=(14, 8))

for i, metric in enumerate(metrics):
    plt.bar([p + i * 0.2 for p in x], results_df[metric], width=0.2, label=metric)

plt.xlabel('Models')
plt.ylabel('Score')
plt.title('Comparison of Model Performance')
plt.xticks([p + 0.3 for p in x], results_df['Model'])
plt.legend()
plt.show()

fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Random Forest Confusion Matrix
axes[0].set_title('Random Forest')
ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=['Focus', 'Anxiety', 'Happy']).plot(ax=axes[0], cmap='Blues')

# LSTM Confusion Matrix
axes[1].set_title('LSTM')
ConfusionMatrixDisplay(confusion_matrix=cm_lstm, display_labels=['Focus', 'Anxiety', 'Happy']).plot(ax=axes[1], cmap='Blues')

# Combined Model Confusion Matrix
axes[2].set_title('Combined LSTM + RF')
ConfusionMatrixDisplay(confusion_matrix=cm_combined, display_labels=['Focus', 'Anxiety', 'Happy']).plot(ax=axes[2], cmap='Blues')

plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np

# Simulate Ambient Temperature
np.random.seed(42)
df['Ambient_TEMP'] = np.random.uniform(15, 40, size=len(df))

# Simulate Weather Conditions
weather_conditions = ['Sunny', 'Rainy', 'Cloudy', 'Windy']
df['Weather'] = np.random.choice(weather_conditions, size=len(df))

# Simulate Indoor/Outdoor Flag
df['Is_Indoor'] = np.random.choice([0, 1], size=len(df))

# Adjust Sensor TEMP
df['Adjusted_TEMP'] = df['TEMP'] - (df['Ambient_TEMP'] * 0.1)

features = ['EDA', 'HRV', 'Adjusted_TEMP', 'Is_Indoor', 'Weather']
X = pd.get_dummies(df[features])  # Convert weather into one-hot encoding
y = df['Emotion']

print(df[['Ambient_TEMP', 'Weather', 'Is_Indoor']].head())
df['Weather'].value_counts().plot(kind='bar')
plt.title('Weather Distribution')
plt.show()

import requests

def get_weather_data(lat, lon):
    api_key = 'fbab7143a27fed1db609931b7e2d4093'  # Replace with your actual API key
    url = f"http://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={api_key}"
    response = requests.get(url)

    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        data = response.json()
        # Check if 'main' and other required keys exist in the response
        if 'main' in data and 'temp' in data['main'] and 'weather' in data and 'main' in data['weather'][0]:
            return {
                'Ambient_TEMP': data['main']['temp'] - 273.15,  # Kelvin to Celsius
                'Weather': data['weather'][0]['main'],
                'Humidity': data['main']['humidity']
            }
        else:
            print("Error: Unexpected API response format. Missing required keys.")
            return None
    else:
        print(f"Error: API request failed with status code {response.status_code}")
        print(response.text)  # Print the error message from the API
        return None

# Example Usage
weather_data = get_weather_data(37.7749, -122.4194)
print(weather_data)

import requests

def get_weather_data(lat, lon):
    api_key = "fbab7143a27fed1db609931b7e2d4093"
    url = f"http://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={api_key}"
    response = requests.get(url)

    if response.status_code == 401:
        print("❌ Error 401: Invalid API Key. Please check your API key.")
        return None
    elif response.status_code != 200:
        print(f"❌ API Request Failed with Status Code: {response.status_code}")
        return None

    data = response.json()
    return {
        'Ambient_TEMP': data['main']['temp'] - 273.15,  # Kelvin to Celsius
        'Weather': data['weather'][0]['main'],
        'Humidity': data['main']['humidity']
    }

# Example Usage
weather_data = get_weather_data(37.7749, -122.4194)
print(weather_data)

import numpy as np

# Assuming latitude and longitude ranges
latitude_range = (37, 38)  # Example range for San Francisco
longitude_range = (-123, -122)  # Example range for San Francisco

# Generate random latitude and longitude within the ranges
df['Latitude'] = np.random.uniform(latitude_range[0], latitude_range[1], size=len(df))
df['Longitude'] = np.random.uniform(longitude_range[0], longitude_range[1], size=len(df))

# Now you can proceed with your code to fetch weather data
from concurrent.futures import ThreadPoolExecutor

unique_locations = df[['Latitude', 'Longitude']].drop_duplicates()

# ... (rest of your code for fetching and using weather data)

import numpy as np

# Assuming latitude and longitude ranges
latitude_range = (37, 38)  # Example range for San Francisco
longitude_range = (-123, -122)  # Example range for San Francisco

# Generate random latitude and longitude within the ranges
df['Latitude'] = np.random.uniform(latitude_range[0], latitude_range[1], size=len(df))
df['Longitude'] = np.random.uniform(longitude_range[0], longitude_range[1], size=len(df))

# Now you can proceed with your code to fetch weather data
from concurrent.futures import ThreadPoolExecutor

unique_locations = df[['Latitude', 'Longitude']].drop_duplicates()

# ... (rest of your code for fetching and using weather data)

import pandas as pd

# Assuming 'df' has a 'Weather' column with raw weather conditions
features = ['EDA', 'HRV', 'Adjusted_TEMP', 'Ambient_TEMP', 'Weather']
# Include 'Weather' for one-hot encoding

# Create one-hot encoded features for 'Weather'
X = pd.get_dummies(df[features], columns=['Weather'])
# Specify 'columns' for one-hot encoding

y = df['Emotion']
print(X.columns) # Print columns to verify one-hot encoding

import pandas as pd

# Assuming 'df' has a 'Weather' column with raw weather conditions
features = ['EDA', 'HRV', 'Adjusted_TEMP', 'Ambient_TEMP', 'Weather']
# Include 'Weather' for one-hot encoding

# Create one-hot encoded features for 'Weather'
X = pd.get_dummies(df[features], columns=['Weather'])
# Specify 'columns' for one-hot encoding

y = df['Emotion']
print(X.columns) # Print columns to verify one-hot encoding

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Model Accuracy with Environmental Data:", accuracy)

# prompt: # Save balanced dataset and
# # Save Random Forest model

# ... (Your existing code)

# Step 10: Save Balanced Dataset
balanced_df.to_csv('/content/drive/MyDrive/WESAD/balancedd_dataset.csv', index=False)

# Step 11: Save Random Forest model (already present in your code)
joblib.dump(rf_model, '/content/drive/MyDrive/WESAD/random_forestz_model.pkl')

# Print the number of features and feature importance size
print("Number of features in dataset (X.columns):", len(X.columns))
print("Number of feature importances from RF:", len(rf_model.feature_importances_))

# Print feature names and importance values
for col, importance in zip(X.columns, rf_model.feature_importances_):
    print(f"Feature: {col}, Importance: {importance}")

# Print feature names expected by the model
print("Model expects features:", rf_model.feature_names_in_)
print("Current dataset features:", X.columns)

# Aggregate Statistical Features for EDA, HRV, TEMP
import pandas as pd
import numpy as np

# Assuming data is time-series, split into windows (e.g., every 10 samples)
window_size = 10

# Create statistical features
def extract_stats(series):
    return {
        'mean': series.mean(),
        'std': series.std(),
        'max': series.max(),
        'min': series.min()
    }

# Apply window-based feature extraction
eda_stats = X['EDA'].rolling(window=window_size).apply(lambda x: extract_stats(x)['mean']).dropna()
hrv_stats = X['HRV'].rolling(window=window_size).apply(lambda x: extract_stats(x)['mean']).dropna()
temp_stats = X['Adjusted_TEMP'].rolling(window=window_size).apply(lambda x: extract_stats(x)['mean']).dropna()

# Combine into a new dataset
df_features = pd.DataFrame({
    'EDA_mean': eda_stats,
    'EDA_std': X['EDA'].rolling(window=window_size).std().dropna(),
    'EDA_max': X['EDA'].rolling(window=window_size).max().dropna(),
    'EDA_min': X['EDA'].rolling(window=window_size).min().dropna(),
    'HRV_mean': hrv_stats,
    'HRV_std': X['HRV'].rolling(window=window_size).std().dropna(),
    'HRV_max': X['HRV'].rolling(window=window_size).max().dropna(),
    'HRV_min': X['HRV'].rolling(window=window_size).min().dropna(),
    'TEMP_mean': X['Adjusted_TEMP'].rolling(window=window_size).mean().dropna(),
    'TEMP_std': X['Adjusted_TEMP'].rolling(window=window_size).std().dropna(),
    'TEMP_max': X['Adjusted_TEMP'].rolling(window=window_size).max().dropna(),
    'TEMP_min': X['Adjusted_TEMP'].rolling(window=window_size).min().dropna(),
}).dropna()

# Ensure alignment with model input
X_final = df_features.reset_index(drop=True)

# Validate the columns
print("Final dataset features:", X_final.columns)
print("Model expects features:", rf_model.feature_names_in_)

# Sample a smaller subset for SHAP
X_sample = X_final.sample(n=100, random_state=42)

# SHAP Explainer
import shap
explainer = shap.TreeExplainer(rf_model)
shap_values = explainer.shap_values(X_sample)

# SHAP Summary Plot
shap.summary_plot(shap_values, X_sample)

import pickle
import pandas as pd
import numpy as np
from scipy.stats import mode
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import warnings

warnings.filterwarnings('ignore')

# Define Sample IDs
sample_ids = ['S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11']
results = []

for sample_id in sample_ids:
    print(f"\n🔄 Processing Sample: {sample_id}")

    # Load Data
    file_path = f'/content/drive/MyDrive/WESAD/WESAD/{sample_id}/{sample_id}.pkl'
    with open(file_path, 'rb') as f:
        data = pickle.load(f, encoding='latin1')

    df = pd.DataFrame({
        'EDA': data['signal']['chest']['EDA'].flatten(),
        'HRV': data['signal']['chest']['ECG'].flatten(),
        'TEMP': data['signal']['chest']['Temp'].flatten(),
        'Label': data['label']
    })

    # Map Labels to Emotions
    emotion_mapping = {1: 'Focus', 2: 'Anxiety', 3: 'Happy'}
    df['Emotion'] = df['Label'].map(emotion_mapping)
    df = df.dropna(subset=['Emotion'])

    # Encode Emotions
    le = LabelEncoder()
    df['Emotion_Encoded'] = le.fit_transform(df['Emotion'])

    # Feature Aggregation
    window_size = 60  # Samples per window
    step_size = 30    # Overlapping step

    features = []
    labels = []

    for start in range(0, len(df) - window_size, step_size):
        window = df.iloc[start:start + window_size]

        if len(window) == window_size:
            features.append({
                'EDA_mean': window['EDA'].mean(),
                'EDA_std': window['EDA'].std(),
                'EDA_max': window['EDA'].max(),
                'EDA_min': window['EDA'].min(),
                'HRV_mean': window['HRV'].mean(),
                'HRV_std': window['HRV'].std(),
                'HRV_max': window['HRV'].max(),
                'HRV_min': window['HRV'].min(),
                'TEMP_mean': window['TEMP'].mean(),
                'TEMP_std': window['TEMP'].std(),
                'TEMP_max': window['TEMP'].max(),
                'TEMP_min': window['TEMP'].min()
            })

            # Handle mode extraction robustly
            window_mode = mode(window['Emotion_Encoded'], keepdims=False)
            # Fix: Check if mode.mode exists and is not empty before accessing [0]
            label = window_mode.mode[0] if hasattr(window_mode, 'mode') and isinstance(window_mode.mode, np.ndarray) and len(window_mode.mode) > 0 else window['Emotion_Encoded'].iloc[0]
            labels.append(int(label))

    # Convert to DataFrame
    X = pd.DataFrame(features) # Fixed indentation
    y = pd.Series(labels).astype(int)  # Ensure labels are integers

    # Validate Shapes
    print("✅ Number of Features:", X.shape)
    print("✅ Number of Labels:", y.shape)
    assert len(X) == len(y), "Feature and Label sizes do not match!"

    # Train-Test Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # Standardize Data
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Train Random Forest Model
    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_model.fit(X_train, y_train)
    y_pred_rf = rf_model.predict(X_test)
    rf_accuracy = accuracy_score(y_test, y_pred_rf)
    print("🌳 Random Forest Accuracy:", rf_accuracy)

    # Train LSTM Model
    X_train_lstm = np.expand_dims(X_train, axis=1)
    X_test_lstm = np.expand_dims(X_test, axis=1)

    lstm_model = Sequential([
        LSTM(32, input_shape=(1, X_train.shape[1])),
        Dropout(0.2),
        Dense(16, activation='relu'),
        Dense(len(np.unique(y)), activation='softmax')
    ])
    lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    lstm_model.fit(X_train_lstm, y_train, epochs=10, batch_size=16, verbose=0)
    lstm_accuracy = lstm_model.evaluate(X_test_lstm, y_test, verbose=0)[1]
    print("🤖 LSTM Accuracy:", lstm_accuracy)

    # Hybrid Model
    X['Adjusted_TEMP'] = X['TEMP_mean'] - (np.mean(df['TEMP']) * 0.1)
    rf_hybrid_model = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_hybrid_model.fit(X_train, y_train)
    y_pred_hybrid = rf_hybrid_model.predict(X_test)
    hybrid_accuracy = accuracy_score(y_test, y_pred_hybrid)
    print("⚡ Hybrid Model Accuracy:", hybrid_accuracy)

    results.append({
        'Sample': sample_id,
        'Random Forest Accuracy': rf_accuracy,
        'LSTM Accuracy': lstm_accuracy,
        'Hybrid Accuracy': hybrid_accuracy
    })

# Save Results
results_df = pd.DataFrame(results)
print("\n📊 Final Results Across Samples:")
print(results_df)
results_df.to_csv('model_results_10_samples.csv', index=False)

import pickle
import pandas as pd
import numpy as np
from scipy.stats import mode
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import warnings

warnings.filterwarnings('ignore')

# Define Sample IDs
sample_ids = ['S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11']

# Store results
results = []

import pickle
import pandas as pd
import numpy as np
from scipy.stats import mode
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import warnings

warnings.filterwarnings('ignore')

# Define Sample IDs
sample_ids = ['S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11']
results = []

for sample_id in sample_ids:
    print(f"\n🔄 Processing Sample: {sample_id}")

    # Load Data
    file_path = f'/content/drive/MyDrive/WESAD/WESAD/{sample_id}/{sample_id}.pkl'
    with open(file_path, 'rb') as f:
        data = pickle.load(f, encoding='latin1')

    df = pd.DataFrame({
        'EDA': data['signal']['chest']['EDA'].flatten(),
        'HRV': data['signal']['chest']['ECG'].flatten(),
        'TEMP': data['signal']['chest']['Temp'].flatten(),
        'Label': data['label']
    })

    # Map Labels to Emotions
    emotion_mapping = {1: 'Focus', 2: 'Anxiety', 3: 'Happy'}
    df['Emotion'] = df['Label'].map(emotion_mapping)
    df = df.dropna(subset=['Emotion'])

    # Encode Emotions
    le = LabelEncoder()
    df['Emotion_Encoded'] = le.fit_transform(df['Emotion'])

    # Feature Aggregation
    window_size = 60  # Samples per window
    step_size = 30    # Overlapping step

    features = []
    labels = []

    for start in range(0, len(df) - window_size, step_size):
        window = df.iloc[start:start + window_size]

        if len(window) == window_size:
            features.append({
                'EDA_mean': window['EDA'].mean(),
                'EDA_std': window['EDA'].std(),
                'EDA_max': window['EDA'].max(),
                'EDA_min': window['EDA'].min(),
                'HRV_mean': window['HRV'].mean(),
                'HRV_std': window['HRV'].std(),
                'HRV_max': window['HRV'].max(),
                'HRV_min': window['HRV'].min(),
                'TEMP_mean': window['TEMP'].mean(),
                'TEMP_std': window['TEMP'].std(),
                'TEMP_max': window['TEMP'].max(),
                'TEMP_min': window['TEMP'].min()
            })

            # Handle mode extraction robustly
            window_mode = mode(window['Emotion_Encoded'], keepdims=False)
            if isinstance(window_mode.mode, (np.ndarray, list)):
                label = window_mode.mode[0] if len(window_mode.mode) > 0 else window['Emotion_Encoded'].iloc[0]
            else:
                label = window_mode
            labels.append(int(label))

    # Convert to DataFrame
    X = pd.DataFrame(features)
    y = pd.Series(labels).astype(int)  # Ensure labels are integers

    # Validate Shapes
    print("✅ Number of Features:", X.shape)
    print("✅ Number of Labels:", y.shape)
    assert len(X) == len(y), "Feature and Label sizes do not match!"

    # Train-Test Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # Standardize Data
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Train Random Forest Model
    rf_model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42
    )
    rf_model.fit(X_train, y_train)
    y_pred_rf = rf_model.predict(X_test)
    rf_accuracy = accuracy_score(y_test, y_pred_rf)
    print("🌳 Random Forest Accuracy:", rf_accuracy)
    print(classification_report(y_test, y_pred_rf))

    # Train LSTM Model
    X_train_lstm = np.expand_dims(X_train, axis=1)
    X_test_lstm = np.expand_dims(X_test, axis=1)
    lstm_model = Sequential([
        LSTM(32, input_shape=(1, X_train.shape[1])),
        Dropout(0.4),
        Dense(16, activation='relu'),
        Dropout(0.3),
        Dense(len(np.unique(y)), activation='softmax')
    ])
    lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    lstm_model.fit(X_train_lstm, y_train, epochs=50, batch_size=32, verbose=0)
    lstm_accuracy = lstm_model.evaluate(X_test_lstm, y_test, verbose=0)[1]
    print("🤖 LSTM Accuracy:", lstm_accuracy)

    # Save Results
    results.append({
        'Sample': sample_id,
        'Random Forest Accuracy': rf_accuracy,
        'LSTM Accuracy': lstm_accuracy
    })

# Save Results
results_df = pd.DataFrame(results)
print("\n📊 Final Results Across Samples:")
print(results_df)
results_df.to_csv('model_results_corrected.csv', index=False)

import pickle
import pandas as pd
import numpy as np
from scipy.stats import mode
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import warnings

warnings.filterwarnings('ignore')

# Define Sample IDs
sample_ids = ['S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11']
results = []

for sample_id in sample_ids:
    print(f"\n🔄 Processing Sample: {sample_id}")

    # Load Data
    file_path = f'/content/drive/MyDrive/WESAD/WESAD/{sample_id}/{sample_id}.pkl'
    with open(file_path, 'rb') as f:
        data = pickle.load(f, encoding='latin1')

    df = pd.DataFrame({
        'EDA': data['signal']['chest']['EDA'].flatten(),
        'HRV': data['signal']['chest']['ECG'].flatten(),
        'TEMP': data['signal']['chest']['Temp'].flatten(),
        'Label': data['label']
    })

    # Map Labels to Emotions
    emotion_mapping = {1: 'Focus', 2: 'Anxiety', 3: 'Happy'}
    df['Emotion'] = df['Label'].map(emotion_mapping)
    df = df.dropna(subset=['Emotion'])

    # Encode Emotions
    le = LabelEncoder()
    df['Emotion_Encoded'] = le.fit_transform(df['Emotion'])

    # Feature Aggregation
    window_size = 60  # Samples per window
    step_size = 30    # Overlapping step

    features = []
    labels = []

    for start in range(0, len(df) - window_size, step_size):
        window = df.iloc[start:start + window_size]

        if len(window) == window_size:
            features.append({
                'EDA_mean': window['EDA'].mean(),
                'EDA_std': window['EDA'].std(),
                'EDA_max': window['EDA'].max(),
                'EDA_min': window['EDA'].min(),
                'HRV_mean': window['HRV'].mean(),
                'HRV_std': window['HRV'].std(),
                'HRV_max': window['HRV'].max(),
                'HRV_min': window['HRV'].min(),
                'TEMP_mean': window['TEMP'].mean(),
                'TEMP_std': window['TEMP'].std(),
                'TEMP_max': window['TEMP'].max(),
                'TEMP_min': window['TEMP'].min()
            })

            # Handle mode extraction robustly
            window_mode = mode(window['Emotion_Encoded'], keepdims=False)
            if hasattr(window_mode, 'mode') and isinstance(window_mode.mode, (np.ndarray, list)):
                label = window_mode.mode[0] if len(window_mode.mode) > 0 else window['Emotion_Encoded'].iloc[0]
            elif isinstance(window_mode, (np.int64, int)):
                label = window_mode
            else:
                label = window['Emotion_Encoded'].iloc[0]
            labels.append(int(label))

    # Convert to DataFrame
    X = pd.DataFrame(features)
    y = pd.Series(labels).astype(int)  # Ensure labels are integers

    # Validate Shapes
    print("✅ Number of Features:", X.shape)
    print("✅ Number of Labels:", y.shape)
    assert len(X) == len(y), "Feature and Label sizes do not match!"

    # Train-Test Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # Standardize Data
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # Train Random Forest Model
    rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
    rf_model.fit(X_train, y_train)
    y_pred_rf = rf_model.predict(X_test)
    rf_accuracy = accuracy_score(y_test, y_pred_rf)
    print("🌳 Random Forest Accuracy:", rf_accuracy)

    # Train LSTM Model
    X_train_lstm = np.expand_dims(X_train, axis=1)
    X_test_lstm = np.expand_dims(X_test, axis=1)
    lstm_model = Sequential([
        LSTM(32, input_shape=(1, X_train.shape[1])),
        Dropout(0.4),
        Dense(16, activation='relu'),
        Dense(len(np.unique(y)), activation='softmax')
    ])
    lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    lstm_model.fit(X_train_lstm, y_train, epochs=10, batch_size=16, verbose=0)
    lstm_accuracy = lstm_model.evaluate(X_test_lstm, y_test, verbose=0)[1]
    print("🤖 LSTM Accuracy:", lstm_accuracy)

    results.append({
        'Sample': sample_id,
        'Random Forest Accuracy': rf_accuracy,
        'LSTM Accuracy': lstm_accuracy
    })

# Save Results
results_df = pd.DataFrame(results)
results_df.to_csv('model_results_corrected.csv', index=False)
print("\n📊 Final Results Across Samples:")
print(results_df)

import pickle
import pandas as pd
import numpy as np
from scipy.stats import mode
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import warnings

warnings.filterwarnings('ignore')

# Define Sample IDs
sample_ids = ['S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11']
results = []

for sample_id in sample_ids:
    print(f"\n🔄 Processing Sample: {sample_id}")

    # Load Data
    file_path = f'/content/drive/MyDrive/WESAD/WESAD/{sample_id}/{sample_id}.pkl'
    with open(file_path, 'rb') as f:
        data = pickle.load(f, encoding='latin1')

    df = pd.DataFrame({
        'EDA': data['signal']['chest']['EDA'].flatten(),
        'HRV': data['signal']['chest']['ECG'].flatten(),
        'TEMP': data['signal']['chest']['Temp'].flatten(),
        'Label': data['label']
    })

    # Map Labels to Emotions
    emotion_mapping = {1: 'Focus', 2: 'Anxiety', 3: 'Happy'}
    df['Emotion'] = df['Label'].map(emotion_mapping)
    df = df.dropna(subset=['Emotion'])

    # Encode Emotions
    le = LabelEncoder()
    df['Emotion_Encoded'] = le.fit_transform(df['Emotion'])

    # Feature Aggregation
    window_size = 60  # Samples per window
    step_size = 30    # Overlapping step

    features = []
    labels = []

    for start in range(0, len(df) - window_size, step_size):
        window = df.iloc[start:start + window_size]

        if len(window) == window_size:
            features.append({
                'EDA_mean': window['EDA'].mean(),
                'EDA_std': window['EDA'].std(),
                'EDA_max': window['EDA'].max(),
                'EDA_min': window['EDA'].min(),
                'HRV_mean': window['HRV'].mean(),
                'HRV_std': window['HRV'].std(),
                'HRV_max': window['HRV'].max(),
                'HRV_min': window['HRV'].min(),
                'TEMP_mean': window['TEMP'].mean(),
                'TEMP_std': window['TEMP'].std(),
                'TEMP_max': window['TEMP'].max(),
                'TEMP_min': window['TEMP'].min()
            })

            # Extract mode for labels
            window_mode = mode(window['Emotion_Encoded'], keepdims=False)
            label = window_mode.mode if hasattr(window_mode, 'mode') and not isinstance(window_mode.mode, np.int64) else window['Emotion_Encoded'].iloc[0] # Use 'isinstance' to check for numpy.int64 type before taking len()



    # Convert to DataFrame
    X = pd.DataFrame(features)
    y = pd.Series(labels).astype(int)

    # Validate Shapes
    print("✅ Number of Features:", X.shape)
    print("✅ Number of Labels:", y.shape)
    assert len(X) == len(y), "Feature and Label sizes do not match!"

    # Train-Test Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # Standardize Data
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    ## 🌳 Random Forest Model
    rf_model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42
    )
    rf_model.fit(X_train, y_train)
    y_pred_rf = rf_model.predict(X_test)
    rf_accuracy = accuracy_score(y_test, y_pred_rf)
    print("🌳 Random Forest Accuracy:", rf_accuracy)

    ## 🤖 LSTM Model
    X_train_lstm = np.expand_dims(X_train, axis=1)
    X_test_lstm = np.expand_dims(X_test, axis=1)
    lstm_model = Sequential([
        LSTM(32, input_shape=(1, X_train.shape[1])),
        Dropout(0.4),
        Dense(16, activation='relu'),
        Dense(len(np.unique(y)), activation='softmax')
    ])
    lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    lstm_model.fit(X_train_lstm, y_train, epochs=10, batch_size=16, verbose=0)
    lstm_accuracy = lstm_model.evaluate(X_test_lstm, y_test, verbose=0)[1]
    print("🤖 LSTM Test Accuracy:", lstm_accuracy)

    ## ⚡ Hybrid Model
    X['Adjusted_TEMP'] = X['TEMP_mean'] - (np.mean(df['TEMP']) * 0.1)
    rf_hybrid_model = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_hybrid_model.fit(X_train, y_train)
    y_pred_hybrid = rf_hybrid_model.predict(X_test)
    hybrid_accuracy = accuracy_score(y_test, y_pred_hybrid)
    print("⚡ Combined Model Accuracy:", hybrid_accuracy)

    ## Store Results
    results.append({
        'Sample': sample_id,
        'Random Forest Accuracy': rf_accuracy,
        'LSTM Test Accuracy': lstm_accuracy,
        'Combined Model Accuracy': hybrid_accuracy
    })

# Save Results
results_df = pd.DataFrame(results)
results_df.to_csv('model_results_10_samples.csv', index=False)
print("\n📊 Final Results Across Samples:")
print(results_df)